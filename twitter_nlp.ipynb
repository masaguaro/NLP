{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, string, glob\n",
    "import platform\n",
    "from collections import Counter\n",
    "from twitter_client import get_twitter_client\n",
    "from tweepy import Cursor\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from selectfile import FileBrowser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_twitter_client()\n",
    "#print(client.me())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_user = input('Please introduce the twitter user name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twitter_user)\n",
    "cwd = os.getcwd()\n",
    "if os.path.isdir(os.path.join(cwd,'raw_data')):\n",
    "    fname = os.path.join(cwd, 'raw_data', 'user_timeline_{}.jsonl'.format(twitter_user)) \n",
    "else:\n",
    "    fname = os.path.join(os.mkdir(os.path.join(cwd,'raw_data')), 'user_timeline_{}.jsonl'.format(twitter_user ))   \n",
    "    \n",
    "print(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the user timeline to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, 'w') as f:\n",
    "    for page in Cursor(client.user_timeline, screen_name=twitter_user, count=200).pages(16):\n",
    "        for status in page:\n",
    "            f.write(json.dumps(status._json) + \"\\n\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_file = FileBrowser()\n",
    "raw_data_file.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_file\n",
    "raw_data_file.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text, tokenizer=TweetTokenizer(), stopwords=[]):\n",
    "    \"\"\"Process the text of a tweet:\n",
    "    - Lowercase\n",
    "    - Tokenize\n",
    "    - Stopword removal\n",
    "    - Digits removal\n",
    "    Return: list of strings\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [tok for tok in tokens if tok not in stopwords and not tok.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "punct = list(string.punctuation)\n",
    "stopword_list = stopwords.words('english') + punct + ['rt', 'via', '...']\n",
    "tf = Counter()\n",
    "with open(raw_data_file.path, 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = process(text=tweet['text'], tokenizer=tweet_tokenizer, stopwords=stopword_list)\n",
    "        tf.update(tokens)\n",
    "for tag, count in tf.most_common(20):\n",
    "    print(\"{}: {}\".format(tag, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geeting data from a set of .txt files contained in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89469534910e47929da3eb6a88b87113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP</h2>'), Button(description='..', style=ButtonSâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data_file = FileBrowser()\n",
    "raw_data_file.widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rodolfo\\Desktop\\NLP\\public_raw_data\\aclImdb\\train\\neg\n",
      "C:\\Users\\Rodolfo\\Desktop\\NLP\\public_raw_data\\aclImdb\\train\\neg\\*.txt\n"
     ]
    }
   ],
   "source": [
    "documents_name_list = glob.glob(os.path.join(raw_data_file.path, \"*.txt\"))\n",
    "print(raw_data_file.path)\n",
    "print(os.path.join(raw_data_file.path, \"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP\\\\public_raw_data\\\\aclImdb\\\\train\\\\neg\\\\0_3.txt', 'C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP\\\\public_raw_data\\\\aclImdb\\\\train\\\\neg\\\\10000_4.txt', 'C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP\\\\public_raw_data\\\\aclImdb\\\\train\\\\neg\\\\10001_4.txt', 'C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP\\\\public_raw_data\\\\aclImdb\\\\train\\\\neg\\\\10002_1.txt', 'C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP\\\\public_raw_data\\\\aclImdb\\\\train\\\\neg\\\\10003_1.txt', 'C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP\\\\public_raw_data\\\\aclImdb\\\\train\\\\neg\\\\10004_3.txt', 'C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP\\\\public_raw_data\\\\aclImdb\\\\train\\\\neg\\\\10005_3.txt', 'C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP\\\\public_raw_data\\\\aclImdb\\\\train\\\\neg\\\\10006_4.txt', 'C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP\\\\public_raw_data\\\\aclImdb\\\\train\\\\neg\\\\10007_1.txt', 'C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP\\\\public_raw_data\\\\aclImdb\\\\train\\\\neg\\\\10008_2.txt']\n"
     ]
    }
   ],
   "source": [
    "print(documents_name_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
