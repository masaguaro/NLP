{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, string\n",
    "import platform\n",
    "from collections import Counter\n",
    "from twitter_client import get_twitter_client\n",
    "from tweepy import Cursor\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from selectfile import FileBrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_twitter_client()\n",
    "#print(client.me())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_user = input('Please introduce the twitter user name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(twitter_user)\n",
    "# cwd = os.getcwd()\n",
    "# if os.path.isdir(os.path.join(cwd,'raw_data')):\n",
    "#     fname = os.path.join(cwd, 'raw_data', 'user_timeline_{}.jsonl'.format(twitter_user)) \n",
    "# else:\n",
    "#     fname = os.path.join(os.mkdir(os.path.join(cwd,'raw_data')), 'user_timeline_{}.jsonl'.format(twitter_user ))   \n",
    "    \n",
    "# print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(fname, 'w') as f:\n",
    "#     for page in Cursor(client.user_timeline, screen_name=twitter_user, count=200).pages(16):\n",
    "#         for status in page:\n",
    "#             f.write(json.dumps(status._json) + \"\\n\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30895140a78846b38e6018a4f7509ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP</h2>'), Button(description='..', style=ButtonS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data_file = FileBrowser()\n",
    "raw_data_file.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Rodolfo\\\\Desktop\\\\NLP\\\\raw_data\\\\user_timeline_jeremycorbyn.jsonl'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_file\n",
    "raw_data_file.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text, tokenizer=TweetTokenizer(), stopwords=[]):\n",
    "    \"\"\"Process the text of a tweet:\n",
    "    - Lowercase\n",
    "    - Tokenize\n",
    "    - Stopword removal\n",
    "    - Digits removal\n",
    "    Return: list of strings\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [tok for tok in tokens if tok not in stopwords and not tok.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "…: 2295\n",
      "labour: 728\n",
      "’: 433\n",
      "today: 429\n",
      "people: 330\n",
      "government: 313\n",
      "@uklabour: 300\n",
      "tories: 247\n",
      "nhs: 210\n",
      "@theresa_may: 199\n",
      "country: 166\n",
      "us: 166\n",
      "great: 153\n",
      "tory: 144\n",
      "vote: 143\n",
      "#forthemany: 131\n",
      "must: 130\n",
      "years: 129\n",
      "campaign: 127\n",
      "many: 115\n"
     ]
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "punct = list(string.punctuation)\n",
    "stopword_list = stopwords.words('english') + punct + ['rt', 'via', '...']\n",
    "tf = Counter()\n",
    "with open(raw_data_file.path, 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = process(text=tweet['text'], tokenizer=tweet_tokenizer, stopwords=stopword_list)\n",
    "        tf.update(tokens)\n",
    "for tag, count in tf.most_common(20):\n",
    "    print(\"{}: {}\".format(tag, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
